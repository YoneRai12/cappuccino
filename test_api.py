"""Llama 3.1-latest + Stable Diffusion API ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import asyncio
import aiohttp
import json
from typing import Dict, Any


class APITester:
    """APIãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def test_health(self) -> Dict[str, Any]:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
        async with self.session.get(f"{self.base_url}/health") as response:
            return await response.json()
    
    async def test_llama_chat(self, prompt: str, model: str = "llama-3.1-latest") -> Dict[str, Any]:
        """Llama 3.1-latestãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
        data = {
            "prompt": prompt,
            "model": model,
            "temperature": 0.8,
            "max_tokens": 2048
        }
        
        async with self.session.post(f"{self.base_url}/llm/chat", json=data) as response:
            return await response.json()
    
    async def test_image_generation(self, prompt: str) -> Dict[str, Any]:
        """Stable Diffusionç”»åƒç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        data = {
            "prompt": prompt,
            "negative_prompt": "blurry, low quality",
            "width": 512,
            "height": 512,
            "steps": 20,
            "cfg_scale": 7.5
        }
        
        async with self.session.post(f"{self.base_url}/image/generate", json=data) as response:
            return await response.json()
    
    async def test_generate_with_image(self, prompt: str, model: str = "llama-3.1-latest") -> Dict[str, Any]:
        """Llama 3.1-latest+Stable Diffusionç”»åƒç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        data = {
            "prompt": prompt,
            "model": model,
            "temperature": 0.8,
            "max_tokens": 2048
        }
        
        async with self.session.post(f"{self.base_url}/llm/generate_with_image", json=data) as response:
            return await response.json()
    
    async def test_chat(self, message: str, model: str = "llama-3.1-latest") -> Dict[str, Any]:
        """Llama 3.1-latestã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
        data = {
            "message": message,
            "model": model
        }
        
        async with self.session.post(f"{self.base_url}/chat", json=data) as response:
            return await response.json()
    
    async def test_models(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ãƒ†ã‚¹ãƒˆ"""
        async with self.session.get(f"{self.base_url}/models") as response:
            return await response.json()


async def run_tests():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸš€ Llama 3.1-latest + Stable Diffusion APIãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™...")
    
    async with APITester() as tester:
        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        print("\nğŸ“Š ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯...")
        health = await tester.test_health()
        print(f"ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯çµæœ: {json.dumps(health, indent=2, ensure_ascii=False)}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
        print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«ä¸€è¦§...")
        models = await tester.test_models()
        print(f"åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {json.dumps(models, indent=2, ensure_ascii=False)}")
        
        # Llama 3.1-latestãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
        print("\nğŸ¦™ Llama 3.1-latestãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆ...")
        chat_result = await tester.test_llama_chat("ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã®å¤©æ°—ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚")
        print(f"Llamaãƒãƒ£ãƒƒãƒˆçµæœ: {json.dumps(chat_result, indent=2, ensure_ascii=False)}")
        
        # Llama 3.1-latestã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
        print("\nğŸ’­ Llama 3.1-latestã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆãƒ†ã‚¹ãƒˆ...")
        simple_chat_result = await tester.test_chat("Pythonã§ç°¡å˜ãªè¨ˆç®—æ©Ÿã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
        print(f"Llamaã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆçµæœ: {json.dumps(simple_chat_result, indent=2, ensure_ascii=False)}")
        
        # Stable Diffusionç”»åƒç”Ÿæˆãƒ†ã‚¹ãƒˆ
        print("\nğŸ¨ Stable Diffusionç”»åƒç”Ÿæˆãƒ†ã‚¹ãƒˆ...")
        try:
            image_result = await tester.test_image_generation("ç¾ã—ã„æ¡œã®èŠ±")
            print(f"ç”»åƒç”Ÿæˆçµæœ: æˆåŠŸ (base64ãƒ‡ãƒ¼ã‚¿é•·: {len(image_result.get('image', ''))})")
            print(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {json.dumps(image_result.get('parameters', {}), indent=2, ensure_ascii=False)}")
        except Exception as e:
            print(f"ç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        
        # Llama 3.1-latest+Stable Diffusionç”»åƒç”Ÿæˆãƒ†ã‚¹ãƒˆ
        print("\nğŸ¦™+ğŸ¨ Llama 3.1-latest+Stable Diffusionç”»åƒç”Ÿæˆãƒ†ã‚¹ãƒˆ...")
        try:
            combined_result = await tester.test_generate_with_image("ç¾ã—ã„æ¡œã®èŠ±ã®ç”»åƒã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚ç”»åƒ: æ¡œã®èŠ±")
            print(f"Llama+ç”»åƒç”Ÿæˆçµæœ: {json.dumps(combined_result, indent=2, ensure_ascii=False)}")
        except Exception as e:
            print(f"Llama+ç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    print("\nâœ… ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")


if __name__ == "__main__":
    asyncio.run(run_tests()) 